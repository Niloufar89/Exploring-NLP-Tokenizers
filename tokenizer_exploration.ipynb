{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of Tokenizer's vocabulary (first 20 tokens):\n",
      "{'edmonton': 10522, 'android': 11924, 'inscription': 9315, '##raÃŸe': 27807, 'furious': 9943, '##Å“': 29674, 'sighted': 19985, 'combatants': 26622, '##00': 8889, 'legion': 8009, 'giacomo': 22873, 'transit': 6671, 'wheels': 7787, 'lift': 6336, '##rlin': 19403, 'cyclist': 14199, 'persuaded': 11766, 'metre': 7924, '##physics': 15638, 'ol': 19330}\n",
      "\n",
      "The vocabulary size is 30522\n",
      "\n",
      "Original sentence: Transformers are revolutionizing NLP!\n",
      "[CLS] -> 101\n",
      "[SEP] -> 102\n",
      "Tokens: ['transformers', 'are', 'revolution', '##izing', 'nl', '##p', '!']\n",
      "Token IDs: [101, 19081, 2024, 4329, 6026, 17953, 2361, 999, 102]\n",
      "Decoded sentence: [CLS] transformers are revolutionizing nlp! [SEP]\n",
      "\n",
      "Original sentence with emoji: I love NLP ðŸ˜Š\n",
      "Tokens (emoji handling): ['i', 'love', 'nl', '##p', '[UNK]']\n",
      "\n",
      "Token IDs for paired sentences: tensor([[ 101, 3019, 2653, 6364, 2003, 5875, 1012,  102, 2054, 2079, 2017, 2228,\n",
      "         2055, 2009, 1029,  102]])\n",
      "Token type IDs for paired sentences: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "Attention mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "\n",
      "Padded attention mask for batched sentences: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "\n",
      "Tokens for a longer sentence: ['token', '##ization', 'in', 'nl', '##p', 'is', 'a', 'crucial', 'step', 'for', 'preparing', 'text', 'data', 'for', 'machine', 'learning', 'models', '.']\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Define the model checkpoint (BERT base uncased)\n",
    "checkpoint = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# Display a subset of the tokenizer's vocabulary and its size\n",
    "print(\"Sample of Tokenizer's vocabulary (first 20 tokens):\")\n",
    "vocab_sample = {k: tokenizer.vocab[k] for k in list(tokenizer.vocab)[:20]}\n",
    "print(vocab_sample)\n",
    "print(f'\\nThe vocabulary size is {len(tokenizer.vocab)}')\n",
    "\n",
    "# Tokenizing a sentence\n",
    "sentence = 'Transformers are revolutionizing NLP!'\n",
    "print(f\"\\nOriginal sentence: {sentence}\")\n",
    "print(f'{tokenizer.cls_token} -> {tokenizer.cls_token_id}')\n",
    "print(f'{tokenizer.sep_token} -> {tokenizer.sep_token_id}')\n",
    "\n",
    "# Tokenize the sentence and get the tokens\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(f\"Tokens: {tokens}\")\n",
    "\n",
    "# Encode the sentence into token IDs\n",
    "ids = tokenizer.encode(sentence)\n",
    "print(f\"Token IDs: {ids}\")\n",
    "\n",
    "# Decode the token IDs back to the original sentence\n",
    "decoded_sentence = tokenizer.decode(ids)\n",
    "print(f\"Decoded sentence: {decoded_sentence}\")\n",
    "\n",
    "# Example: Tokenizing a sentence with an emoji (out-of-vocabulary handling)\n",
    "emoji_sentence = 'I love NLP ðŸ˜Š'\n",
    "print(f\"\\nOriginal sentence with emoji: {emoji_sentence}\")\n",
    "emoji_tokens = tokenizer.tokenize(emoji_sentence)\n",
    "print(f\"Tokens (emoji handling): {emoji_tokens}\")\n",
    "\n",
    "# Handling paired sentences\n",
    "first_sentence = 'Natural language processing is interesting.'\n",
    "second_sentence = 'What do you think about it?'\n",
    "input = tokenizer(first_sentence, second_sentence, return_tensors='pt')\n",
    "print(f\"\\nToken IDs for paired sentences: {input['input_ids']}\")\n",
    "print(f\"Token type IDs for paired sentences: {input['token_type_ids']}\")\n",
    "print(f\"Attention mask: {input['attention_mask']}\")\n",
    "\n",
    "# Padding sentences in a batch\n",
    "first_sentence = 'Machine learning models require lots of data.'\n",
    "second_sentence = 'Deep learning models learn through neural networks.'\n",
    "batch_input = tokenizer([first_sentence, second_sentence], padding=True, return_tensors='pt')\n",
    "print(f\"\\nPadded attention mask for batched sentences: {batch_input['attention_mask']}\")\n",
    "\n",
    "# Experiment with tokenizing a longer sentence\n",
    "long_sentence = 'Tokenization in NLP is a crucial step for preparing text data for machine learning models.'\n",
    "long_tokens = tokenizer.tokenize(long_sentence)\n",
    "print(f\"\\nTokens for a longer sentence: {long_tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
