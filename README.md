# Exploring NLP Tokenizers Using Hugging Face Transformers

This project explores the usage of tokenizers from the Hugging Face Transformers library, focusing on how sentences are tokenized, encoded, and decoded in the context of NLP tasks. The project demonstrates how to work with tokenizers using the BERT model and provides detailed insights into the structure of tokenization, vocabulary size, and sentence pairing.

### Objective:
- Learn how tokenizers process text and manage tokens for input into transformer models.
- Understand how sentences are tokenized, converted to token IDs, and transformed into model inputs.
- Explore tokenization in more complex cases, such as sentence pairing and handling special tokens.

### Files:
- **tokenizer_exploration.py**: The Python script that implements tokenization for various text inputs and tasks using the `AutoTokenizer`.

### Installation:
1. Clone the repository:
    ```bash
    git clone https://github.com/yourusername/Exploring-NLP-Tokenizers.git
    ```
2. Install the required dependencies:
    ```bash
    pip install -r requirements.txt
    ```

### How to Run:
Run the script to explore tokenizer behavior:
```bash
python tokenizer_exploration.py
